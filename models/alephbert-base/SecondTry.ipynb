{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': 'כי אתה טיפש', 'score': 0.4186239242553711, 'token': 1919, 'token_str': 'כי'}, {'sequence': 'כמה אתה טיפש', 'score': 0.05661720782518387, 'token': 2146, 'token_str': 'כמה'}, {'sequence': 'גם אתה טיפש', 'score': 0.030103864148259163, 'token': 1929, 'token_str': 'גם'}, {'sequence': 'אז אתה טיפש', 'score': 0.02940039522945881, 'token': 2039, 'token_str': 'אז'}, {'sequence': 'למה אתה טיפש', 'score': 0.02271442301571369, 'token': 2211, 'token_str': 'למה'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers.tokenization_utils import TruncationStrategy\n",
    "\n",
    "import tokenizers\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"AlephBERT-base\": {\n",
    "        \"name_or_path\":\"onlplab/alephbert-base\",\n",
    "        \"description\":\"AlephBERT base model\",\n",
    "    },\n",
    "    \"HeBERT-base-TAU\": {\n",
    "        \"name_or_path\":\"avichr/heBERT\",\n",
    "        \"description\":\"HeBERT model created by TAU\"\n",
    "    },\n",
    "    \"mBERT-base-multilingual-cased\": {\n",
    "        \"name_or_path\":\"bert-base-multilingual-cased\",\n",
    "        \"description\":\"Multilingual BERT model\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_json_from_url(url):\n",
    "    return models\n",
    "    return requests.get(url).json()\n",
    "\n",
    "# models = get_json_from_url('https://huggingface.co/spaces/biu-nlp/AlephBERT/raw/main/models.json')\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model):\n",
    "    pipe = pipeline('fill-mask', models[model]['name_or_path'])\n",
    "    def do_tokenize(inputs):\n",
    "        return pipe.tokenizer(\n",
    "                inputs,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=pipe.framework,\n",
    "                padding=True,\n",
    "                truncation=TruncationStrategy.DO_NOT_TRUNCATE,\n",
    "            )\n",
    "\n",
    "    def _parse_and_tokenize(\n",
    "        inputs, tokenized=False, **kwargs\n",
    "    ):\n",
    "        if not tokenized:\n",
    "            inputs = do_tokenize(inputs)\n",
    "        return inputs\n",
    "\n",
    "    pipe._parse_and_tokenize = _parse_and_tokenize\n",
    "    \n",
    "    return pipe, do_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mode = 'Models'\n",
    "\n",
    "if mode == 'Models':\n",
    "    model = \"AlephBERT-base\"\n",
    "    masking_level = 'Tokens'\n",
    "    \n",
    "    model_tags = model.split('-')\n",
    "    model_tags[0] = 'Model:' + model_tags[0] \n",
    "\n",
    "\n",
    "    unmasker, tokenize = load_model(model)\n",
    "    input_text = \" [MASK] אתה טיפש \"      \n",
    "    input_masked = None\n",
    "    tokenized = tokenize(input_text)\n",
    "    ids = tokenized['input_ids'].tolist()[0]\n",
    "    subwords = unmasker.tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "    if masking_level == 'Tokens':\n",
    "        tokens = str(input_text).split()\n",
    "        mask_idx =  '[MASK]'\n",
    "        if mask_idx is not None:\n",
    "            input_masked = ' '.join(token if i != mask_idx else '[MASK]' for i, token in enumerate(tokens))\n",
    "            display_input = input_masked\n",
    "    if input_masked: \n",
    "        ids = tokenized['input_ids'].tolist()[0]\n",
    "        subwords = unmasker.tokenizer.convert_ids_to_tokens(ids)\n",
    "        res = unmasker(input_masked,  top_k=5)\n",
    "        if res:\n",
    "            print(res)\n",
    "#             res = [{'Prediction':r['token_str'], 'Completed Sentence':r['sequence'].replace('[SEP]', '').replace('[CLS]', ''), 'Score':r['score']} for r in res]\n",
    "#             res_table = pd.DataFrame(res)\n",
    "#             st.table(res_table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'כי אתה טיפש',\n",
       " 'score': 0.4186239242553711,\n",
       " 'token': 1919,\n",
       " 'token_str': 'כי'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "alephbert_tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.alephbert_model = BertModel.from_pretrained(\"onlplab/alephbert-base\")\n",
    "        \n",
    "    def forward(self,ids):\n",
    "#         _,o2= self.alephbert_model(ids,attention_mask=mask,token_type_ids=token_type_ids, return_dict=False)\n",
    "        \n",
    "#         out = self.out(o2)\n",
    "        \n",
    "        return self.alephbert_model(ids)\n",
    "    \n",
    "model=Encoder()\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#Initialize Optimizer\n",
    "optimizer= optim.Adam(model.parameters(),lr= 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-e4f7623a2a5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malephbert_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel_dec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-e4f7623a2a5b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malephbert_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"onlplab/alephbert-base\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mmodeling_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModuleUtilsMixin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_extended_attention_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mget_extended_attention_mask\u001b[1;34m(self, attention_mask, input_shape, device)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;31m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m             \u001b[0mextended_attention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.alephbert_model = BertModel.from_pretrained(\"onlplab/alephbert-base\")\n",
    "        #modeling_utils.ModuleUtilsMixin.get_extended_attention_mask(self,[0],3,device)\n",
    "\n",
    "    def forward(self,ids):\n",
    "#         _,o2= self.alephbert_model(ids,attention_mask=mask,token_type_ids=token_type_ids, return_dict=False)\n",
    "        \n",
    "#         out = self.out(o2)\n",
    "        \n",
    "        return self.alephbert_model(ids)\n",
    "    \n",
    "model_dec=Decoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─BertEmbeddings: 1-1                         --\n",
      "|    └─Embedding: 2-1                         39,936,000\n",
      "|    └─Embedding: 2-2                         393,216\n",
      "|    └─Embedding: 2-3                         768\n",
      "|    └─LayerNorm: 2-4                         1,536\n",
      "|    └─Dropout: 2-5                           --\n",
      "├─BertEncoder: 1-2                            --\n",
      "|    └─ModuleList: 2-6                        --\n",
      "|    |    └─BertLayer: 3-1                    7,087,872\n",
      "|    |    └─BertLayer: 3-2                    7,087,872\n",
      "|    |    └─BertLayer: 3-3                    7,087,872\n",
      "|    |    └─BertLayer: 3-4                    7,087,872\n",
      "|    |    └─BertLayer: 3-5                    7,087,872\n",
      "|    |    └─BertLayer: 3-6                    7,087,872\n",
      "|    |    └─BertLayer: 3-7                    7,087,872\n",
      "|    |    └─BertLayer: 3-8                    7,087,872\n",
      "|    |    └─BertLayer: 3-9                    7,087,872\n",
      "|    |    └─BertLayer: 3-10                   7,087,872\n",
      "|    |    └─BertLayer: 3-11                   7,087,872\n",
      "|    |    └─BertLayer: 3-12                   7,087,872\n",
      "├─BertPooler: 1-3                             --\n",
      "|    └─Linear: 2-7                            590,592\n",
      "|    └─Tanh: 2-8                              --\n",
      "======================================================================\n",
      "Total params: 125,976,576\n",
      "Trainable params: 125,976,576\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─BertEmbeddings: 1-1                         --\n",
       "|    └─Embedding: 2-1                         39,936,000\n",
       "|    └─Embedding: 2-2                         393,216\n",
       "|    └─Embedding: 2-3                         768\n",
       "|    └─LayerNorm: 2-4                         1,536\n",
       "|    └─Dropout: 2-5                           --\n",
       "├─BertEncoder: 1-2                            --\n",
       "|    └─ModuleList: 2-6                        --\n",
       "|    |    └─BertLayer: 3-1                    7,087,872\n",
       "|    |    └─BertLayer: 3-2                    7,087,872\n",
       "|    |    └─BertLayer: 3-3                    7,087,872\n",
       "|    |    └─BertLayer: 3-4                    7,087,872\n",
       "|    |    └─BertLayer: 3-5                    7,087,872\n",
       "|    |    └─BertLayer: 3-6                    7,087,872\n",
       "|    |    └─BertLayer: 3-7                    7,087,872\n",
       "|    |    └─BertLayer: 3-8                    7,087,872\n",
       "|    |    └─BertLayer: 3-9                    7,087,872\n",
       "|    |    └─BertLayer: 3-10                   7,087,872\n",
       "|    |    └─BertLayer: 3-11                   7,087,872\n",
       "|    |    └─BertLayer: 3-12                   7,087,872\n",
       "├─BertPooler: 1-3                             --\n",
       "|    └─Linear: 2-7                            590,592\n",
       "|    └─Tanh: 2-8                              --\n",
       "======================================================================\n",
       "Total params: 125,976,576\n",
       "Trainable params: 125,976,576\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(BertModel.from_pretrained(\"onlplab/alephbert-base\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcd8e8eac7a41a4bc4b3998d71d6283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=505.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7f9449571844d4929b63d2a6da78ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=438146887.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at avichr/heBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─BertEmbeddings: 1-1                         --\n",
      "|    └─Embedding: 2-1                         23,440,896\n",
      "|    └─Embedding: 2-2                         393,216\n",
      "|    └─Embedding: 2-3                         1,536\n",
      "|    └─LayerNorm: 2-4                         1,536\n",
      "|    └─Dropout: 2-5                           --\n",
      "├─BertEncoder: 1-2                            --\n",
      "|    └─ModuleList: 2-6                        --\n",
      "|    |    └─BertLayer: 3-1                    7,087,872\n",
      "|    |    └─BertLayer: 3-2                    7,087,872\n",
      "|    |    └─BertLayer: 3-3                    7,087,872\n",
      "|    |    └─BertLayer: 3-4                    7,087,872\n",
      "|    |    └─BertLayer: 3-5                    7,087,872\n",
      "|    |    └─BertLayer: 3-6                    7,087,872\n",
      "|    |    └─BertLayer: 3-7                    7,087,872\n",
      "|    |    └─BertLayer: 3-8                    7,087,872\n",
      "|    |    └─BertLayer: 3-9                    7,087,872\n",
      "|    |    └─BertLayer: 3-10                   7,087,872\n",
      "|    |    └─BertLayer: 3-11                   7,087,872\n",
      "|    |    └─BertLayer: 3-12                   7,087,872\n",
      "├─BertPooler: 1-3                             --\n",
      "|    └─Linear: 2-7                            590,592\n",
      "|    └─Tanh: 2-8                              --\n",
      "======================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─BertEmbeddings: 1-1                         --\n",
       "|    └─Embedding: 2-1                         23,440,896\n",
       "|    └─Embedding: 2-2                         393,216\n",
       "|    └─Embedding: 2-3                         1,536\n",
       "|    └─LayerNorm: 2-4                         1,536\n",
       "|    └─Dropout: 2-5                           --\n",
       "├─BertEncoder: 1-2                            --\n",
       "|    └─ModuleList: 2-6                        --\n",
       "|    |    └─BertLayer: 3-1                    7,087,872\n",
       "|    |    └─BertLayer: 3-2                    7,087,872\n",
       "|    |    └─BertLayer: 3-3                    7,087,872\n",
       "|    |    └─BertLayer: 3-4                    7,087,872\n",
       "|    |    └─BertLayer: 3-5                    7,087,872\n",
       "|    |    └─BertLayer: 3-6                    7,087,872\n",
       "|    |    └─BertLayer: 3-7                    7,087,872\n",
       "|    |    └─BertLayer: 3-8                    7,087,872\n",
       "|    |    └─BertLayer: 3-9                    7,087,872\n",
       "|    |    └─BertLayer: 3-10                   7,087,872\n",
       "|    |    └─BertLayer: 3-11                   7,087,872\n",
       "|    |    └─BertLayer: 3-12                   7,087,872\n",
       "├─BertPooler: 1-3                             --\n",
       "|    └─Linear: 2-7                            590,592\n",
       "|    └─Tanh: 2-8                              --\n",
       "======================================================================\n",
       "Total params: 109,482,240\n",
       "Trainable params: 109,482,240\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(BertModel.from_pretrained(\"avichr/heBERT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, tokenizer,max_length):\n",
    "        super(BertDataset, self).__init__()\n",
    "        self.root_dir=root_dir\n",
    "        self.train_csv=pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n",
    "        self.tokenizer=tokenizer\n",
    "        self.target=self.train_csv.iloc[:,1]\n",
    "        self.max_length=max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text1 = self.train_csv.iloc[index,0]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text1 ,\n",
    "            None,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'target': torch.tensor(self.train_csv.iloc[index, 1], dtype=torch.long)\n",
    "            }\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "dataset= BertDataset(tokenizer, max_length=100)\n",
    "\n",
    "dataloader=DataLoader(dataset=dataset,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_review = alephbert_tokenizer.encode_plus(\n",
    "\"[MASK] הלכתי אתמול\",\n",
    "return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "input_ids = encoded_review['input_ids'].to(device)\n",
    "output = model(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([742])\n"
     ]
    }
   ],
   "source": [
    "_, prediction = torch.max(output[1], dim=1)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import modeling_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_extended_attention_mask() missing 4 required positional arguments: 'self', 'attention_mask', 'input_shape', and 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-4d4c89bb5498>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodeling_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModuleUtilsMixin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_extended_attention_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: get_extended_attention_mask() missing 4 required positional arguments: 'self', 'attention_mask', 'input_shape', and 'device'"
     ]
    }
   ],
   "source": [
    "modeling_utils.ModuleUtilsMixin.get_extended_attention_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
